{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: loading the necessary libraries and start the graph session\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting temp\\train-images-idx3-ubyte.gz\n",
      "Extracting temp\\train-labels-idx1-ubyte.gz\n",
      "Extracting temp\\t10k-images-idx3-ubyte.gz\n",
      "Extracting temp\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# step 2: load the data and transform  he images into 28*28 arrays\n",
    "data_dir='temp'\n",
    "mnist=read_data_sets(data_dir)\n",
    "train_xdata=np.array([np.reshape(x, (28, 28)) for x in mnist.train.images])\n",
    "test_xdata=np.array([np.reshape(x, (28, 28)) for x in mnist.test.images ])\n",
    "train_labels=mnist.train.labels\n",
    "test_labels=mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "(55000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_xdata.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step:3 define the model hyper-parameters \n",
    "batch_size=100\n",
    "learning_rate=0.05\n",
    "evaluation_size=100\n",
    "image_width=train_xdata[0].shape[0]\n",
    "image_height=train_xdata[0].shape[1]\n",
    "target_size= 1\n",
    "num_channel=1\n",
    "generations=500\n",
    "eval_every=5\n",
    "conv1_features=28\n",
    "conv2_features=50\n",
    "max_pool_size1=2\n",
    "max_pool_size2=2\n",
    "fully_connected_size1=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step: 4 decleare placeholder for the data.\n",
    "x_input_shape=(batch_size, num_channel, image_width, image_height)\n",
    "x_input=tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target=tf.placeholder(tf.int32, shape=(batch_size, 1))\n",
    "eval_input_shape=(evaluation_size, num_channel, image_width, image_height)\n",
    "eval_input=tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target=tf.placeholder(tf.int32, shape=(batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: declare weight and bias Variables for CNN network\n",
    "conv1_weight=tf.Variable(tf.truncated_normal([4, 4,  conv1_features, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias=tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "conv2_weight=tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features], stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias=tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6: declare weight and bias for fully connected network \n",
    "resulting_width=image_width//(max_pool_size1* max_pool_size2)\n",
    "resulting_height=image_height//(max_pool_size1*max_pool_size2)\n",
    "full1_input_size=1*resulting_width*conv2_features\n",
    "full1_weight=tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full1_bias=tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full2_weight=tf.Variable(tf.truncated_normal([fully_connected_size1, target_size], stddev=0.1, dtype=tf.float32))\n",
    "full2_bias=tf.Variable(tf.truncated_normal([1], stddev=0.1, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 7: declaring our entire CNN framework. (CNN + fully connected layers model)\n",
    "def my_conv_net(input_data):\n",
    "    # First Conv-ReLU-MaxPool Layer\n",
    "    conv1=tf.nn.conv2d(input_data, conv1_weight, strides=[1,1,1,1], padding='SAME')\n",
    "    relu1=tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1=tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1], strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "    \n",
    "    # second Conv_ReLU_Maxpool layer\n",
    "    conv2=tf.nn.conv2d(max_pool1, conv2_weight, strides=[1,1,1,1], padding='SAME')\n",
    "    relu2=tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2=tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1], strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n",
    "    print(max_pool2.shape)\n",
    "    # Transform output from CNN layer to Fully connceted layer\n",
    "    \n",
    "    final_conv_shape=max_pool2.get_shape().as_list()\n",
    "    final_shape=final_conv_shape[1]*final_conv_shape[2]*final_conv_shape[3]\n",
    "    print(final_conv_shape[0])\n",
    "    flat_output=tf.reshape(max_pool2, [fully_connected_size1, final_shape])\n",
    "\n",
    "    # First fully connected Layer\n",
    "    fully_connected1=tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "    final_model_output=tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n",
    "    \n",
    "    return (final_model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 7, 50)\n",
      "100\n",
      "(100, 1)\n",
      "(100, 1, 7, 50)\n",
      "100\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "(100, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# step 8: declare model with train and test data. apply loss and accuracy function\n",
    "model_output=my_conv_net(x_input)\n",
    "print(model_output.shape)\n",
    "test_model_output=my_conv_net(eval_input)\n",
    "print(test_model_output.shape)\n",
    "print(y_target.shape)\n",
    "print(x_input.shape)\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_target, logits=model_output))\n",
    "\n",
    "# # accuracy function\n",
    "prediction=tf.nn.softmax(model_output)\n",
    "test_prediction=tf.nn.softmax(test_model_output)\n",
    "\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions=np.argmax(logits, axis=1)\n",
    "    num_correct=np.sum(np.equal(batch_predictions, targets))\n",
    "    return (100. * num_correct/batch_predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 9: define optimizer function and initilize model variables\n",
    "my_optimizer=tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_step=my_optimizer.minimize(loss)\n",
    "\n",
    "# initialize variables\n",
    "init=tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation # 5. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 10. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 15. Train loss: nan. Train Acc (Test Acc):500.00(0.00)\n",
      "Generation # 20. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 25. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 30. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 35. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 40. Train loss: nan. Train Acc (Test Acc):1900.00(0.00)\n",
      "Generation # 45. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 50. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 55. Train loss: nan. Train Acc (Test Acc):300.00(0.00)\n",
      "Generation # 60. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 65. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 70. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 75. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 80. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 85. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 90. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 95. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 100. Train loss: nan. Train Acc (Test Acc):1400.00(0.00)\n",
      "Generation # 105. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 110. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 115. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 120. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 125. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 130. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 135. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 140. Train loss: nan. Train Acc (Test Acc):1700.00(0.00)\n",
      "Generation # 145. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 150. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 155. Train loss: nan. Train Acc (Test Acc):1600.00(0.00)\n",
      "Generation # 160. Train loss: nan. Train Acc (Test Acc):500.00(0.00)\n",
      "Generation # 165. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 170. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 175. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 180. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 185. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 190. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 195. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 200. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 205. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 210. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 215. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 220. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 225. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 230. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 235. Train loss: nan. Train Acc (Test Acc):1400.00(0.00)\n",
      "Generation # 240. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 245. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 250. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 255. Train loss: nan. Train Acc (Test Acc):500.00(0.00)\n",
      "Generation # 260. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 265. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 270. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 275. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 280. Train loss: nan. Train Acc (Test Acc):1400.00(0.00)\n",
      "Generation # 285. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 290. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 295. Train loss: nan. Train Acc (Test Acc):1600.00(100.00)\n",
      "Generation # 300. Train loss: nan. Train Acc (Test Acc):500.00(0.00)\n",
      "Generation # 305. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 310. Train loss: nan. Train Acc (Test Acc):1600.00(0.00)\n",
      "Generation # 315. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 320. Train loss: nan. Train Acc (Test Acc):1300.00(0.00)\n",
      "Generation # 325. Train loss: nan. Train Acc (Test Acc):1300.00(0.00)\n",
      "Generation # 330. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 335. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 340. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 345. Train loss: nan. Train Acc (Test Acc):1600.00(0.00)\n",
      "Generation # 350. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 355. Train loss: nan. Train Acc (Test Acc):400.00(0.00)\n",
      "Generation # 360. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 365. Train loss: nan. Train Acc (Test Acc):500.00(0.00)\n",
      "Generation # 370. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 375. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 380. Train loss: nan. Train Acc (Test Acc):1500.00(0.00)\n",
      "Generation # 385. Train loss: nan. Train Acc (Test Acc):600.00(0.00)\n",
      "Generation # 390. Train loss: nan. Train Acc (Test Acc):500.00(0.00)\n",
      "Generation # 395. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 400. Train loss: nan. Train Acc (Test Acc):1600.00(0.00)\n",
      "Generation # 405. Train loss: nan. Train Acc (Test Acc):1500.00(0.00)\n",
      "Generation # 410. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 415. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 420. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 425. Train loss: nan. Train Acc (Test Acc):1200.00(0.00)\n",
      "Generation # 430. Train loss: nan. Train Acc (Test Acc):1300.00(0.00)\n",
      "Generation # 435. Train loss: nan. Train Acc (Test Acc):700.00(0.00)\n",
      "Generation # 440. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 445. Train loss: nan. Train Acc (Test Acc):1300.00(0.00)\n",
      "Generation # 450. Train loss: nan. Train Acc (Test Acc):1300.00(0.00)\n",
      "Generation # 455. Train loss: nan. Train Acc (Test Acc):900.00(0.00)\n",
      "Generation # 460. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n",
      "Generation # 465. Train loss: nan. Train Acc (Test Acc):1300.00(0.00)\n",
      "Generation # 470. Train loss: nan. Train Acc (Test Acc):400.00(100.00)\n",
      "Generation # 475. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 480. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 485. Train loss: nan. Train Acc (Test Acc):1400.00(0.00)\n",
      "Generation # 490. Train loss: nan. Train Acc (Test Acc):1100.00(0.00)\n",
      "Generation # 495. Train loss: nan. Train Acc (Test Acc):800.00(0.00)\n",
      "Generation # 500. Train loss: nan. Train Acc (Test Acc):1000.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "#step 10: starting Training model\n",
    "train_loss=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "for i in range(generations):\n",
    "    rand_index = np.random.choice(len(train_xdata), size=(batch_size))\n",
    "    rand_x=train_xdata[rand_index]\n",
    "    rand_x=np.expand_dims(rand_x,1) \n",
    "    rand_y=train_labels[rand_index]\n",
    "    rand_y=np.expand_dims(rand_y,1)\n",
    "    train_dict= {x_input:rand_x, y_target: rand_y}\n",
    "    #print(rand_x.shape)\n",
    "    #print(rand_y.shape)\n",
    "    sess.run(train_step,  feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds=sess.run([loss, prediction], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "    \n",
    "    if (i+1)% eval_every==0:\n",
    "        eval_index=np.random.choice(len(test_xdata), size=(evaluation_size))\n",
    "       # print(len(eval_index))\n",
    "        eval_x=test_xdata[eval_index]\n",
    "        eval_x=np.expand_dims(eval_x, 1)\n",
    "        eval_y=test_labels[eval_index]\n",
    "        eval_y=np.expand_dims(eval_index, 1)\n",
    "        test_dict={eval_input:eval_x, eval_target:eval_y}\n",
    "        test_preds=sess.run(test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc=get_accuracy(test_preds, eval_y)\n",
    "        \n",
    "        # record and print results\n",
    "        train_loss.append(temp_train_loss)\n",
    "        test_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    "        acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train loss: {:.2f}. Train Acc (Test Acc):{:.2f}({:.2f})'.format(*acc_and_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
